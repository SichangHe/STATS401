{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpGyR-YI5b0G"
   },
   "source": [
    "#Import Tweets Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJoupv7u5en8"
   },
   "source": [
    "\n",
    "1.   Import Tweets from sntwitter API (https://github.com/JustAnotherArchivist/snscrape)\n",
    "\n",
    "2.   Clean up data - Remove special characters, emojis, memes in each tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3430,
     "status": "ok",
     "timestamp": 1648752399662,
     "user": {
      "displayName": "Ziqiao Ao",
      "userId": "12315067596244745838"
     },
     "user_tz": -480
    },
    "id": "TL4hnDjRfBdJ",
    "outputId": "4cfa627c-f3eb-4da1-a8ad-c523a6f2c287"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "import matplotlib.pyplot as mlpt\n",
    "import csv\n",
    "!pip install snscrape\n",
    "import datetime as dt\n",
    "import snscrape.modules.twitter as sntwitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18795,
     "status": "ok",
     "timestamp": 1648752487686,
     "user": {
      "displayName": "Ziqiao Ao",
      "userId": "12315067596244745838"
     },
     "user_tz": -480
    },
    "id": "HkOBULwofJWg"
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('#Black Lives Matter  since:2022-01-01','corona lang:en').get_items()):\n",
    "    tweets.append([tweet.date,  tweet.content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1648752487687,
     "user": {
      "displayName": "Ziqiao Ao",
      "userId": "12315067596244745838"
     },
     "user_tz": -480
    },
    "id": "NnN8TgDXxroy"
   },
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame(tweets, columns=['Date', 'Tweets'])\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1648752487688,
     "user": {
      "displayName": "Ziqiao Ao",
      "userId": "12315067596244745838"
     },
     "user_tz": -480
    },
    "id": "76LJY2Z-vxaU"
   },
   "outputs": [],
   "source": [
    "data=tweets\n",
    "cdata=pd.DataFrame(columns=['Date','Tweets'])\n",
    "index=0\n",
    "for index,row in data.iterrows():\n",
    "    stre=row[\"Tweets\"]\n",
    "    #the following cleaning steps are not the standard. You may generate your own ways of cleaning.\n",
    "    my_new_string = re.sub('#\\w+', '', stre) #remove the hashtags\n",
    "    my_new_string = re.sub('http\\S+', '', my_new_string) #remove the links\n",
    "    my_new_string = re.sub('\\n', '', my_new_string) #remove \\n\n",
    "    cdata.sort_index()\n",
    "    cdata.at[index,'Date']=row[\"Date\"]\n",
    "    cdata.at[index,'Tweets']=my_new_string\n",
    "    index=index+1\n",
    "tweets=cdata\n",
    "tweets['Date'] = tweets['Date'].astype('datetime64[ns]')\n",
    "tweets[\"Date\"]=tweets[\"Date\"].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1648752487688,
     "user": {
      "displayName": "Ziqiao Ao",
      "userId": "12315067596244745838"
     },
     "user_tz": -480
    },
    "id": "61s2fPu6TT9c",
    "outputId": "cd746c88-d871-4709-86df-128867ebab22"
   },
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N13nzV7Bzmdz"
   },
   "source": [
    "#Sentiment Analysis for Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N78XHzHZ7YTD"
   },
   "source": [
    "Sentiment Analysis is a process of ‘computationally’ evaluating whether a piece of text is positive, negative or neutral. In our case, conducting sentiment analysis on tweets help determine the public's moods towards BLM(Black Lives Matter) topic.\n",
    "1.   **VADER (Valence Aware Dictionary and Entiment Reasoner)** https://github.com/cjhutto/vaderSentiment\n",
    "\n",
    "```\n",
    "\"VADER is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed\n",
    "in social media. A sentiment lexicon is a list of lexical features (e.g., words) which are generally labeled\n",
    "according to their semantic orientation aseither positive or negative. VADER not only tells about the Positivity\n",
    "and Negativity score but also tells us about how positive or negative a sentiment is.\" (GreeksforGreeks)\n",
    "```\n",
    "2.   **Reference**\n",
    " \n",
    "  Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51627,
     "status": "ok",
     "timestamp": 1648752551155,
     "user": {
      "displayName": "Ziqiao Ao",
      "userId": "12315067596244745838"
     },
     "user_tz": -480
    },
    "id": "_OdcFhCfhbO4",
    "outputId": "3203cc36-3a37-4ad5-c2d1-91aad9cb61c7"
   },
   "outputs": [],
   "source": [
    "!pip install vaderSentiment\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7353,
     "status": "ok",
     "timestamp": 1648752558504,
     "user": {
      "displayName": "Ziqiao Ao",
      "userId": "12315067596244745838"
     },
     "user_tz": -480
    },
    "id": "4_1Od8XAhgQk"
   },
   "outputs": [],
   "source": [
    "import pyspark as spark\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col,udf,monotonically_increasing_id,unix_timestamp,round,avg\n",
    "import re\n",
    "sc = spark.SparkContext()\n",
    "sql = spark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12371,
     "status": "ok",
     "timestamp": 1648752570872,
     "user": {
      "displayName": "Ziqiao Ao",
      "userId": "12315067596244745838"
     },
     "user_tz": -480
    },
    "id": "-8Rnh3gShik7",
    "outputId": "cacea988-cd77-474f-f648-d995ae5b45c6"
   },
   "outputs": [],
   "source": [
    "FullDataTw=sql.createDataFrame(tweets)\n",
    "FullDataTw = FullDataTw.dropna()\n",
    "FullDataTw.select(monotonically_increasing_id().alias(\"rowId\"),\"*\")\n",
    " #setting column names of Twitter dataset\n",
    "CleanDF = FullDataTw.withColumnRenamed('Tweets', 'Tweet')\n",
    "CleanDF = FullDataTw.withColumnRenamed('Date', 'Date_Time')\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "def senti_score_udf(sentence):\n",
    "    snt = analyser.polarity_scores(sentence)\n",
    "    return ([snt['neg'], snt['neu'], snt['pos'], snt['compound']])\n",
    "func_udf2 = udf(senti_score_udf, ArrayType(FloatType()))\n",
    "CleanDF = CleanDF.withColumn('p_neg', func_udf2(CleanDF['Tweets'])[0])  #withColumn function would generate the result with a new column name\n",
    "CleanDF = CleanDF.withColumn('p_neu', func_udf2(CleanDF['Tweets'])[1])\n",
    "CleanDF = CleanDF.withColumn('p_pos', func_udf2(CleanDF['Tweets'])[2])\n",
    "CleanDF = CleanDF.withColumn('p_comp', func_udf2(CleanDF['Tweets'])[3])\n",
    "CleanDF.show(120)\n",
    "CleanDF.toPandas().to_csv('sen.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXZJepzI8Idx"
   },
   "source": [
    " **Categorize Sentiments**\n",
    "Once getting the compound sentiment score for each Tweets, we put them into categories. \n",
    "\n",
    "\n",
    "  $$Negative \\ Sentiment : -0.9<P_{comp}\\leq -0.05$$\n",
    "   \n",
    "  $$Positive\\  Sentiment: 0.05\\leq P_{comp}<0.9$$\n",
    "\n",
    "  $$Neutral \\ Sentiment:  -0.05<P_{comp}<0.05 $$\n",
    "\n",
    "  $$Extremely\\ Negative \\ Sentiment:  P_{comp}\\leq -0.9 $$\n",
    "\n",
    "  $$Extremely\\ Postive \\ Sentiment:  P_{comp}\\geq 0.9 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1648752570872,
     "user": {
      "displayName": "Ziqiao Ao",
      "userId": "12315067596244745838"
     },
     "user_tz": -480
    },
    "id": "YxlRRHuqVSMi"
   },
   "outputs": [],
   "source": [
    "tw=pd.read_csv(\"sen.csv\")\n",
    "tw[\"length\"]=\"\"\n",
    "i=0\n",
    "# iterate through the csv file \n",
    "for val in tw[\"Tweets\"]: \n",
    "  val = str(val) \n",
    "  tokens = val.split() \n",
    "  tw[\"length\"][i]=len(tokens)\n",
    "  i=i+1\n",
    "tw=tw.drop(columns=[\"p_neg\",\"p_neu\",\"p_pos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1648752570873,
     "user": {
      "displayName": "Ziqiao Ao",
      "userId": "12315067596244745838"
     },
     "user_tz": -480
    },
    "id": "VWyJS528X2oJ"
   },
   "outputs": [],
   "source": [
    "tw.loc[(tw['p_comp']>-0.05)&(tw['p_comp']<0.05),'sentiment']=\"neutral\"\n",
    "tw.loc[(tw['p_comp']>=0.05)&(tw['p_comp']<0.9),'sentiment']='positive'\n",
    "tw.loc[(tw['p_comp']<=-0.05)&(tw['p_comp']>-0.9),'sentiment']='negative'\n",
    "tw.loc[tw['p_comp']<=-0.9,'sentiment']='extremely negative'\n",
    "tw.loc[tw['p_comp']>=0.9,'sentiment']='extremely positive'\n",
    "tw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ELSoAKoUW4Q"
   },
   "source": [
    "# Exercise2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gfQy_3OUbWU"
   },
   "source": [
    "#### Crawl data from twitter with another topic and do sentiment analysis on the data, after you get the results, try to use graphs to visualize the sentiment analysis results (any type of graph that could satisfy your purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sccvfTwUw2W"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Notebook2_Sentiment_Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "285c4757dfc4e94b079e369a02af7e2629906879163375b2ee80895267f4cf04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e19baa18",
   "metadata": {
    "id": "e19baa18"
   },
   "source": [
    "# Scraping Twitter data with snscrape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558419b9",
   "metadata": {
    "id": "558419b9"
   },
   "source": [
    "## 0. Set up\n",
    "**snscrape** is a scraper for social networking services. It currently supports lots of services such as Twitter, Facebook, Instagram, etc. It scrapes things like user profiles, hashtags, or searches and returns the discovered items, e.g. the relevant posts. More information about snscrape can be viewed from its [GitHub](https://github.com/JustAnotherArchivist/snscrape) site.\n",
    "\n",
    "**Note: you may need to connect the Duke vpn (portal.duke.edu) to finish this lab.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e934ed1",
   "metadata": {
    "id": "9e934ed1",
    "outputId": "cebabb46-2a68-45ae-c9fa-cf40d924779d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snscrape in /opt/homebrew/lib/python3.10/site-packages (0.5.0.20230113)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from snscrape) (3.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/homebrew/lib/python3.10/site-packages (from snscrape) (4.11.1)\n",
      "Requirement already satisfied: requests[socks] in /opt/homebrew/lib/python3.10/site-packages (from snscrape) (2.28.2)\n",
      "Requirement already satisfied: lxml in /opt/homebrew/lib/python3.10/site-packages (from snscrape) (4.9.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/lib/python3.10/site-packages (from beautifulsoup4->snscrape) (2.3.2.post1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests[socks]->snscrape) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests[socks]->snscrape) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests[socks]->snscrape) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests[socks]->snscrape) (2022.12.7)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/homebrew/lib/python3.10/site-packages (from requests[socks]->snscrape) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "# install snscrape using pip\n",
    "#important notice: require Python version 3.8+\n",
    "#if <3.8, try \"conda update python=[target version]\" or create new conda env with \"conda create -n [name you like] python=[target version]\" in command line\n",
    "!pip3 install snscrape\n",
    "\n",
    "#or\n",
    "#pip3 install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
    "\n",
    "#more details please refer to the github link: https://github.com/JustAnotherArchivist/snscrape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4c6e7d6",
   "metadata": {
    "id": "d4c6e7d6"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import snscrape.modules.twitter\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e58a2f",
   "metadata": {
    "id": "b0e58a2f"
   },
   "source": [
    "## 1. Overview\n",
    "\n",
    "In today's lab, we are going to use snscrape to scrape data from twitter. You will learn to scrape tweets from:\n",
    "1. text search query\n",
    "2. a specific user\n",
    "\n",
    "Snscrape provides many useful attributes available through snscrape Twitter object [(Beck, 2020)](https://betterprogramming.pub/how-to-scrape-tweets-with-snscrape-90124ed006af). You might need some of the above attributes when scraping data for your group projects.\n",
    "* url: Permalink pointing to tweet location\n",
    "* date: The date on which the tweet was created\n",
    "* content: The text content of the tweet\n",
    "* id: The id of the tweet\n",
    "* user: User object containing the following data: username, displayname, id, description, descriptionUrls, verified, created, followersCount, friendsCount, statusesCount, favouritesCount, listedCount, location, protected, linkUrl, profileImageUrl, profileBannerUrl\n",
    "* replyCount: The count of replies\n",
    "* retweetCount: The count of retweets\n",
    "* likeCount: The count of likes\n",
    "* quoteCount: The count of users that quoted the tweet and replied\n",
    "* conversationId: Appears to be the same as tweet id\n",
    "* lang: Machine generated, assumed language of the tweet\n",
    "* source: Where tweet was posted from (e.g., iPhone, Andriod, etc.)\n",
    "* retweetedTweet: The id of the original tweet (if it is a retweet)\n",
    "* mentionedUsers: The user objects of any mentioned user in the tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ac6c0f",
   "metadata": {
    "id": "81ac6c0f"
   },
   "source": [
    "##  2. Scrape tweets from a text search query\n",
    "\n",
    "The queried text could be certain keywords and hashtags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6bf7a5",
   "metadata": {
    "id": "4a6bf7a5"
   },
   "source": [
    "###  Example 1\n",
    "In this example, we scrape 10 tweets with the **#stopasianhate** hashtag with at least three likes `min_faves:3`. We save the scraped tweets with its posted date `tweet.date`, text content `tweet.content`, poster's location `tweet.user.location`, and the number of received likes `tweet.likeCount` into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcd716e6",
   "metadata": {
    "id": "bcd716e6"
   },
   "outputs": [],
   "source": [
    "# create a list to append twitter data to\n",
    "tweets_list1 = []\n",
    "\n",
    "# use TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('#stopasianhate since:2021-03-15 until:2021-03-16 min_faves:3').get_items()):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    tweets_list1.append([tweet.date,  tweet.rawContent, tweet.user.location, tweet.likeCount])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86960147",
   "metadata": {
    "id": "86960147",
    "outputId": "cfcda3fd-f16c-43a6-df57-ef0e14ff0fec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Content</th>\n",
       "      <th>User's location</th>\n",
       "      <th># of likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-15 23:40:23+00:00</td>\n",
       "      <td>Nobody can stop @JoeBiden @FLOTUS ! #vaccine n...</td>\n",
       "      <td>Âè∞ÂåóÂ∏Ç, Âè∞ÁÅ£</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-15 23:36:27+00:00</td>\n",
       "      <td>A big thank you to the entire community for su...</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-15 23:28:45+00:00</td>\n",
       "      <td>@BigBosSebas @rayvolpe nice we talking about #...</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-15 23:08:56+00:00</td>\n",
       "      <td>@CeFaanKim @Syissle @JoshHartmann Thank you  @...</td>\n",
       "      <td>Los Angeles &amp; New York</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-15 23:04:32+00:00</td>\n",
       "      <td>1st step to solving a problem is acknowledging...</td>\n",
       "      <td>San Antonio, TX</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-03-15 22:48:26+00:00</td>\n",
       "      <td>Grateful for all my #YeunBuns, but let‚Äôs keep ...</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>1530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-03-15 22:35:14+00:00</td>\n",
       "      <td>We condemn the hate crimes and discrimination ...</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-03-15 22:28:09+00:00</td>\n",
       "      <td>When you have a moment please read this thread...</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-03-15 22:24:12+00:00</td>\n",
       "      <td>‚ÄúGo back to f****** communist China you b****!...</td>\n",
       "      <td>New York City</td>\n",
       "      <td>1078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-03-15 22:21:42+00:00</td>\n",
       "      <td>#StopAsianHate</td>\n",
       "      <td>LA. Westside is the Bestside</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Date  \\\n",
       "0 2021-03-15 23:40:23+00:00   \n",
       "1 2021-03-15 23:36:27+00:00   \n",
       "2 2021-03-15 23:28:45+00:00   \n",
       "3 2021-03-15 23:08:56+00:00   \n",
       "4 2021-03-15 23:04:32+00:00   \n",
       "5 2021-03-15 22:48:26+00:00   \n",
       "6 2021-03-15 22:35:14+00:00   \n",
       "7 2021-03-15 22:28:09+00:00   \n",
       "8 2021-03-15 22:24:12+00:00   \n",
       "9 2021-03-15 22:21:42+00:00   \n",
       "\n",
       "                                             Content  \\\n",
       "0  Nobody can stop @JoeBiden @FLOTUS ! #vaccine n...   \n",
       "1  A big thank you to the entire community for su...   \n",
       "2  @BigBosSebas @rayvolpe nice we talking about #...   \n",
       "3  @CeFaanKim @Syissle @JoshHartmann Thank you  @...   \n",
       "4  1st step to solving a problem is acknowledging...   \n",
       "5  Grateful for all my #YeunBuns, but let‚Äôs keep ...   \n",
       "6  We condemn the hate crimes and discrimination ...   \n",
       "7  When you have a moment please read this thread...   \n",
       "8  ‚ÄúGo back to f****** communist China you b****!...   \n",
       "9                                     #StopAsianHate   \n",
       "\n",
       "                User's location  # of likes  \n",
       "0                       Âè∞ÂåóÂ∏Ç, Âè∞ÁÅ£           4  \n",
       "1             San Francisco, CA           8  \n",
       "2               Los Angeles, CA           5  \n",
       "3        Los Angeles & New York          10  \n",
       "4               San Antonio, TX           4  \n",
       "5               Los Angeles, CA        1530  \n",
       "6                     Worldwide           6  \n",
       "7                   Atlanta, GA          24  \n",
       "8                 New York City        1078  \n",
       "9  LA. Westside is the Bestside          10  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe to view the result\n",
    "tweets_df1 = pd.DataFrame(tweets_list1, columns=['Date', 'Content', 'User\\'s location', '# of likes'])\n",
    "tweets_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4515d5fc",
   "metadata": {
    "id": "4515d5fc"
   },
   "source": [
    "### Example 2\n",
    "\n",
    "In this example, we scrape tweets that contains keywords in the phrase **let's go duke** around the date of a basketball match. We save the scraped tweets with its posted date `tweet.date`, text content `tweet.content`, and the user's location `tweet.user.location` into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "845106a7",
   "metadata": {
    "id": "845106a7"
   },
   "outputs": [],
   "source": [
    "# create a list to append twitter data to\n",
    "tweets_list2 = []\n",
    "\n",
    "# use TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('let\\'s go duke since:2021-11-12 until:2021-11-13').get_items()):\n",
    "    tweets_list2.append([tweet.date, tweet.rawContent, tweet.user.location])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78570e1d",
   "metadata": {
    "id": "78570e1d",
    "outputId": "c871b842-4226-4c99-ce73-e339dc610dac",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Content</th>\n",
       "      <th>User's location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-11-12 23:59:21+00:00</td>\n",
       "      <td>Let's go Duke ü§ò</td>\n",
       "      <td>My Own Personal Hell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-11-12 23:58:46+00:00</td>\n",
       "      <td>Roach\\nKeels\\nMoore\\nBanchero\\nWilliams\\n\\nGra...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-11-12 23:57:29+00:00</td>\n",
       "      <td>üëèüèª Let‚Äôs üëèüèª Go ‚úäüèª Duke! üîµüòà https://t.co/H589Xp...</td>\n",
       "      <td>Germantown, MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-11-12 23:56:40+00:00</td>\n",
       "      <td>Let‚Äôs go Duke!! #BeatArmy</td>\n",
       "      <td>#SI6HTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-11-12 23:55:37+00:00</td>\n",
       "      <td>LET‚ÄôS GO DUKE!!! One last ride against where t...</td>\n",
       "      <td>Raleigh, NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-11-12 23:54:42+00:00</td>\n",
       "      <td>Let‚Äôs Go Dukeüëø https://t.co/beTdrGHzXW</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-11-12 23:53:25+00:00</td>\n",
       "      <td>Let‚Äôs go Duke!!! üíôüòàüèÄ</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-11-12 23:51:58+00:00</td>\n",
       "      <td>LET‚ÄôS GO DUKE üòà</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-11-12 23:51:13+00:00</td>\n",
       "      <td>Gametime baby let's go Duke https://t.co/lmXSV...</td>\n",
       "      <td>San Antonio, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-11-12 23:48:02+00:00</td>\n",
       "      <td>It‚Äôs almost game time Let‚Äôs Go Duke #DukeNation</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-11-12 23:47:43+00:00</td>\n",
       "      <td>@dukepbp @DukeMBB @ReedsJewelers @DukeATHLETIC...</td>\n",
       "      <td>Harrisburg PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-11-12 23:28:17+00:00</td>\n",
       "      <td>@DukeMBB üî•üî•let‚Äôs go duke</td>\n",
       "      <td>Uptown Dc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021-11-12 23:26:28+00:00</td>\n",
       "      <td>@DerrickWalkerJ2 Let's go ACC. Looking forward...</td>\n",
       "      <td>Fort Lauderdale, FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2021-11-12 23:25:53+00:00</td>\n",
       "      <td>let‚Äôs go duke</td>\n",
       "      <td>RGF Island</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021-11-12 23:25:14+00:00</td>\n",
       "      <td>Soooooo Nice to be back in Cameron !!!!! Let‚Äôs...</td>\n",
       "      <td>Durham, NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021-11-12 23:24:31+00:00</td>\n",
       "      <td>Let‚Äôs Go Duke</td>\n",
       "      <td>North KaK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2021-11-12 22:52:40+00:00</td>\n",
       "      <td>@GameDayCharlie @ABC11_WTVD @DukeMBB Let's go ...</td>\n",
       "      <td>The Greater Triangle Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2021-11-12 22:34:52+00:00</td>\n",
       "      <td>GAME DAY !!! #HereComesDuke !!!\\nüèÄüèÄüèÄ‚Ä¶vs Army\\n...</td>\n",
       "      <td>Harrisburg PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021-11-12 22:07:10+00:00</td>\n",
       "      <td>We Have An All New Wwe Smackdown And An All Ae...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-11-12 21:47:08+00:00</td>\n",
       "      <td>@DukeWBB Let's go duke!!!!</td>\n",
       "      <td>in my right mind :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2021-11-12 21:44:02+00:00</td>\n",
       "      <td>@JCrossover @Pp_doesit Me and Paolo B day twin...</td>\n",
       "      <td>206üìç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021-11-12 21:37:59+00:00</td>\n",
       "      <td>Let‚Äôs go Duke !</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2021-11-12 21:03:02+00:00</td>\n",
       "      <td>Duke will be wearing their gray/gold/blue jers...</td>\n",
       "      <td>‚ú®Illinois ‚ú®</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2021-11-12 19:15:46+00:00</td>\n",
       "      <td>@DukeMBB Let‚Äôs GO DUKE !!!</td>\n",
       "      <td>Harrisburg PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2021-11-12 17:39:22+00:00</td>\n",
       "      <td>So stoked to get this Veterans Tribute Showcas...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2021-11-12 16:53:51+00:00</td>\n",
       "      <td>Let‚Äôs Go Duke! #Gameday</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2021-11-12 16:46:14+00:00</td>\n",
       "      <td>@sobberatduke @DukeMBB Let‚Äôs go Duke!</td>\n",
       "      <td>Dallas, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2021-11-12 16:01:21+00:00</td>\n",
       "      <td>Same tournament 2 years ago and still playing ...</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2021-11-12 15:56:33+00:00</td>\n",
       "      <td>Let‚Äôs go Duke! https://t.co/dQvVu3wGe8</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2021-11-12 15:35:46+00:00</td>\n",
       "      <td>Girls‚Äô Diving\\nState\\nKalina‚Äôs first dive!\\nGr...</td>\n",
       "      <td>Elmhurst, IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2021-11-12 15:34:19+00:00</td>\n",
       "      <td>Let‚Äôs go @DaytonWBB‚ÄºÔ∏è\\n\\nBe sure to show up to...</td>\n",
       "      <td>Dayton, OH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2021-11-12 15:33:14+00:00</td>\n",
       "      <td>Let‚Äôs go Duke!</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2021-11-12 15:05:30+00:00</td>\n",
       "      <td>Duke bound!  Here we come, Cameron. Thank you ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2021-11-12 14:55:16+00:00</td>\n",
       "      <td>Happy Birthday Paolo üòàüòàüíôüíô Let‚Äôs go Duke #GameDay</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2021-11-12 14:14:24+00:00</td>\n",
       "      <td>üîµ Gameday! Let's Go Duke! üîµ\\n\\nüèÜ Veterans Day ...</td>\n",
       "      <td>Durham, NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2021-11-12 14:01:05+00:00</td>\n",
       "      <td>@Zlanier21 @sarahlongthorne Dukes up. SJ let‚Äôs...</td>\n",
       "      <td>Bellingham, WA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2021-11-12 13:24:21+00:00</td>\n",
       "      <td>It‚Äôs game day! \\n\\nLet‚Äôs go Duke!</td>\n",
       "      <td>Vincent Ohio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2021-11-12 12:30:54+00:00</td>\n",
       "      <td>@DukeMBB @UpdateDuke @DukeDigest @dukepbp @duk...</td>\n",
       "      <td>World Wide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2021-11-12 07:54:04+00:00</td>\n",
       "      <td>@Qu3stion_84 @DukeOfSexy @rainboom16 @HimuraHi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2021-11-12 02:03:14+00:00</td>\n",
       "      <td>Happy Veterans Day!! First game in Cameron tom...</td>\n",
       "      <td>Cameron Indoor Stadium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2021-11-12 00:29:59+00:00</td>\n",
       "      <td>@TylerMeinerding Kenny Anderson throwback is ü§Æ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Date  \\\n",
       "0  2021-11-12 23:59:21+00:00   \n",
       "1  2021-11-12 23:58:46+00:00   \n",
       "2  2021-11-12 23:57:29+00:00   \n",
       "3  2021-11-12 23:56:40+00:00   \n",
       "4  2021-11-12 23:55:37+00:00   \n",
       "5  2021-11-12 23:54:42+00:00   \n",
       "6  2021-11-12 23:53:25+00:00   \n",
       "7  2021-11-12 23:51:58+00:00   \n",
       "8  2021-11-12 23:51:13+00:00   \n",
       "9  2021-11-12 23:48:02+00:00   \n",
       "10 2021-11-12 23:47:43+00:00   \n",
       "11 2021-11-12 23:28:17+00:00   \n",
       "12 2021-11-12 23:26:28+00:00   \n",
       "13 2021-11-12 23:25:53+00:00   \n",
       "14 2021-11-12 23:25:14+00:00   \n",
       "15 2021-11-12 23:24:31+00:00   \n",
       "16 2021-11-12 22:52:40+00:00   \n",
       "17 2021-11-12 22:34:52+00:00   \n",
       "18 2021-11-12 22:07:10+00:00   \n",
       "19 2021-11-12 21:47:08+00:00   \n",
       "20 2021-11-12 21:44:02+00:00   \n",
       "21 2021-11-12 21:37:59+00:00   \n",
       "22 2021-11-12 21:03:02+00:00   \n",
       "23 2021-11-12 19:15:46+00:00   \n",
       "24 2021-11-12 17:39:22+00:00   \n",
       "25 2021-11-12 16:53:51+00:00   \n",
       "26 2021-11-12 16:46:14+00:00   \n",
       "27 2021-11-12 16:01:21+00:00   \n",
       "28 2021-11-12 15:56:33+00:00   \n",
       "29 2021-11-12 15:35:46+00:00   \n",
       "30 2021-11-12 15:34:19+00:00   \n",
       "31 2021-11-12 15:33:14+00:00   \n",
       "32 2021-11-12 15:05:30+00:00   \n",
       "33 2021-11-12 14:55:16+00:00   \n",
       "34 2021-11-12 14:14:24+00:00   \n",
       "35 2021-11-12 14:01:05+00:00   \n",
       "36 2021-11-12 13:24:21+00:00   \n",
       "37 2021-11-12 12:30:54+00:00   \n",
       "38 2021-11-12 07:54:04+00:00   \n",
       "39 2021-11-12 02:03:14+00:00   \n",
       "40 2021-11-12 00:29:59+00:00   \n",
       "\n",
       "                                              Content  \\\n",
       "0                                     Let's go Duke ü§ò   \n",
       "1   Roach\\nKeels\\nMoore\\nBanchero\\nWilliams\\n\\nGra...   \n",
       "2   üëèüèª Let‚Äôs üëèüèª Go ‚úäüèª Duke! üîµüòà https://t.co/H589Xp...   \n",
       "3                           Let‚Äôs go Duke!! #BeatArmy   \n",
       "4   LET‚ÄôS GO DUKE!!! One last ride against where t...   \n",
       "5              Let‚Äôs Go Dukeüëø https://t.co/beTdrGHzXW   \n",
       "6                                Let‚Äôs go Duke!!! üíôüòàüèÄ   \n",
       "7                                     LET‚ÄôS GO DUKE üòà   \n",
       "8   Gametime baby let's go Duke https://t.co/lmXSV...   \n",
       "9     It‚Äôs almost game time Let‚Äôs Go Duke #DukeNation   \n",
       "10  @dukepbp @DukeMBB @ReedsJewelers @DukeATHLETIC...   \n",
       "11                           @DukeMBB üî•üî•let‚Äôs go duke   \n",
       "12  @DerrickWalkerJ2 Let's go ACC. Looking forward...   \n",
       "13                                      let‚Äôs go duke   \n",
       "14  Soooooo Nice to be back in Cameron !!!!! Let‚Äôs...   \n",
       "15                                      Let‚Äôs Go Duke   \n",
       "16  @GameDayCharlie @ABC11_WTVD @DukeMBB Let's go ...   \n",
       "17  GAME DAY !!! #HereComesDuke !!!\\nüèÄüèÄüèÄ‚Ä¶vs Army\\n...   \n",
       "18  We Have An All New Wwe Smackdown And An All Ae...   \n",
       "19                         @DukeWBB Let's go duke!!!!   \n",
       "20  @JCrossover @Pp_doesit Me and Paolo B day twin...   \n",
       "21                                    Let‚Äôs go Duke !   \n",
       "22  Duke will be wearing their gray/gold/blue jers...   \n",
       "23                         @DukeMBB Let‚Äôs GO DUKE !!!   \n",
       "24  So stoked to get this Veterans Tribute Showcas...   \n",
       "25                            Let‚Äôs Go Duke! #Gameday   \n",
       "26              @sobberatduke @DukeMBB Let‚Äôs go Duke!   \n",
       "27  Same tournament 2 years ago and still playing ...   \n",
       "28             Let‚Äôs go Duke! https://t.co/dQvVu3wGe8   \n",
       "29  Girls‚Äô Diving\\nState\\nKalina‚Äôs first dive!\\nGr...   \n",
       "30  Let‚Äôs go @DaytonWBB‚ÄºÔ∏è\\n\\nBe sure to show up to...   \n",
       "31                                     Let‚Äôs go Duke!   \n",
       "32  Duke bound!  Here we come, Cameron. Thank you ...   \n",
       "33   Happy Birthday Paolo üòàüòàüíôüíô Let‚Äôs go Duke #GameDay   \n",
       "34  üîµ Gameday! Let's Go Duke! üîµ\\n\\nüèÜ Veterans Day ...   \n",
       "35  @Zlanier21 @sarahlongthorne Dukes up. SJ let‚Äôs...   \n",
       "36                  It‚Äôs game day! \\n\\nLet‚Äôs go Duke!   \n",
       "37  @DukeMBB @UpdateDuke @DukeDigest @dukepbp @duk...   \n",
       "38  @Qu3stion_84 @DukeOfSexy @rainboom16 @HimuraHi...   \n",
       "39  Happy Veterans Day!! First game in Cameron tom...   \n",
       "40  @TylerMeinerding Kenny Anderson throwback is ü§Æ...   \n",
       "\n",
       "              User's location  \n",
       "0        My Own Personal Hell  \n",
       "1                              \n",
       "2              Germantown, MD  \n",
       "3                     #SI6HTS  \n",
       "4                 Raleigh, NC  \n",
       "5                              \n",
       "6                              \n",
       "7                         252  \n",
       "8             San Antonio, TX  \n",
       "9                 Atlanta, GA  \n",
       "10              Harrisburg PA  \n",
       "11                  Uptown Dc  \n",
       "12        Fort Lauderdale, FL  \n",
       "13                 RGF Island  \n",
       "14                 Durham, NC  \n",
       "15                  North KaK  \n",
       "16  The Greater Triangle Area  \n",
       "17              Harrisburg PA  \n",
       "18                             \n",
       "19        in my right mind :)  \n",
       "20                       206üìç  \n",
       "21                             \n",
       "22                ‚ú®Illinois ‚ú®  \n",
       "23              Harrisburg PA  \n",
       "24                             \n",
       "25                             \n",
       "26                 Dallas, TX  \n",
       "27                Atlanta, GA  \n",
       "28                             \n",
       "29               Elmhurst, IL  \n",
       "30                 Dayton, OH  \n",
       "31                             \n",
       "32                             \n",
       "33                Atlanta, GA  \n",
       "34                 Durham, NC  \n",
       "35             Bellingham, WA  \n",
       "36               Vincent Ohio  \n",
       "37                 World Wide  \n",
       "38                             \n",
       "39    Cameron Indoor Stadium   \n",
       "40                             "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe to view the result\n",
    "tweets_df2 = pd.DataFrame(tweets_list2, columns=['Date', 'Content', 'User\\'s location'])\n",
    "tweets_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ab0fc4",
   "metadata": {
    "id": "16ab0fc4"
   },
   "source": [
    "## 3. Scrape tweets from a specific user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506268da",
   "metadata": {
    "id": "506268da"
   },
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a6f869",
   "metadata": {
    "id": "f5a6f869"
   },
   "source": [
    "In this example, we scrape the latest 100 tweets posted by the Twitter user **@DukeKunshan**. We save the scraped tweets with its posted date `tweet.date`, text content `tweet.content`, and username `tweet.user.username` into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad8bddc2",
   "metadata": {
    "id": "ad8bddc2"
   },
   "outputs": [],
   "source": [
    "# create a list to append twitter data to\n",
    "tweets_list3 = []\n",
    "\n",
    "# use TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('from:DukeKunshan').get_items()):\n",
    "    if i >= 100:\n",
    "        break\n",
    "    tweets_list3.append([tweet.date, tweet.rawContent, tweet.user.username])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34b178c1",
   "metadata": {
    "id": "34b178c1",
    "outputId": "4f2ae593-6f01-4616-e481-5810c35b1bce",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Content</th>\n",
       "      <th>Username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-11 13:00:29+00:00</td>\n",
       "      <td>Enjoy these photos of DKU's Phase II site, whi...</td>\n",
       "      <td>DukeKunshan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-04 11:30:08+00:00</td>\n",
       "      <td>A student-professor team from Duke Kunshan, le...</td>\n",
       "      <td>DukeKunshan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-02 02:00:10+00:00</td>\n",
       "      <td>Only 1 day left to submit your application to ...</td>\n",
       "      <td>DukeKunshan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-31 13:00:16+00:00</td>\n",
       "      <td>Wishing you a Happy New Year from everyone at ...</td>\n",
       "      <td>DukeKunshan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-28 13:00:11+00:00</td>\n",
       "      <td>A pioneering DKU project aimed at making the w...</td>\n",
       "      <td>DukeKunshan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2022-05-18 03:00:04+00:00</td>\n",
       "      <td>üì¢ Happening tomorrow: HRC Anthropocene XR Lab ...</td>\n",
       "      <td>DukeKunshan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2022-05-18 01:00:05+00:00</td>\n",
       "      <td>üì¢ Happening tomorrow: iMep Virtual Information...</td>\n",
       "      <td>DukeKunshan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2022-05-17 02:00:05+00:00</td>\n",
       "      <td>#DKUcommencement #DKU2022 https://t.co/8abNckowTK</td>\n",
       "      <td>DukeKunshan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2022-05-16 02:00:02+00:00</td>\n",
       "      <td>Duke Kunshan's offices of undergraduate studie...</td>\n",
       "      <td>DukeKunshan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2022-05-15 02:00:01+00:00</td>\n",
       "      <td>Class of 2022  students Lan Tang and Laura Nav...</td>\n",
       "      <td>DukeKunshan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Date  \\\n",
       "0  2023-01-11 13:00:29+00:00   \n",
       "1  2023-01-04 11:30:08+00:00   \n",
       "2  2023-01-02 02:00:10+00:00   \n",
       "3  2022-12-31 13:00:16+00:00   \n",
       "4  2022-12-28 13:00:11+00:00   \n",
       "..                       ...   \n",
       "95 2022-05-18 03:00:04+00:00   \n",
       "96 2022-05-18 01:00:05+00:00   \n",
       "97 2022-05-17 02:00:05+00:00   \n",
       "98 2022-05-16 02:00:02+00:00   \n",
       "99 2022-05-15 02:00:01+00:00   \n",
       "\n",
       "                                              Content     Username  \n",
       "0   Enjoy these photos of DKU's Phase II site, whi...  DukeKunshan  \n",
       "1   A student-professor team from Duke Kunshan, le...  DukeKunshan  \n",
       "2   Only 1 day left to submit your application to ...  DukeKunshan  \n",
       "3   Wishing you a Happy New Year from everyone at ...  DukeKunshan  \n",
       "4   A pioneering DKU project aimed at making the w...  DukeKunshan  \n",
       "..                                                ...          ...  \n",
       "95  üì¢ Happening tomorrow: HRC Anthropocene XR Lab ...  DukeKunshan  \n",
       "96  üì¢ Happening tomorrow: iMep Virtual Information...  DukeKunshan  \n",
       "97  #DKUcommencement #DKU2022 https://t.co/8abNckowTK  DukeKunshan  \n",
       "98  Duke Kunshan's offices of undergraduate studie...  DukeKunshan  \n",
       "99  Class of 2022  students Lan Tang and Laura Nav...  DukeKunshan  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe to view the result\n",
    "tweets_df3 = pd.DataFrame(tweets_list3, columns=['Date', 'Content', 'Username'])\n",
    "tweets_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85c16e4",
   "metadata": {
    "id": "a85c16e4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2eb8c36f",
   "metadata": {
    "id": "2eb8c36f"
   },
   "source": [
    "# Optional: Scraping Weibo data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42b0c56",
   "metadata": {
    "id": "b42b0c56"
   },
   "source": [
    "If your project's topic falls in the environment of Chinese society, you may need to scrape data from [Weibo](http://www.weibo.com/) instead of Twitter. A simple, easy-to-use API can be found [here](https://github.com/dataabc/weibo-search). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d904af5",
   "metadata": {
    "id": "8d904af5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "STATS401_lab3_data_scraper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{"cells":[{"cell_type":"markdown","id":"e19baa18","metadata":{"id":"e19baa18"},"source":["# Scraping Twitter data with snscrape"]},{"cell_type":"markdown","id":"558419b9","metadata":{"id":"558419b9"},"source":["## 0. Set up\n","**snscrape** is a scraper for social networking services. It currently supports lots of services such as Twitter, Facebook, Instagram, etc. It scrapes things like user profiles, hashtags, or searches and returns the discovered items, e.g. the relevant posts. More information about snscrape can be viewed from its [GitHub](https://github.com/JustAnotherArchivist/snscrape) site.\n","\n","**Note: you may need to connect the Duke vpn (portal.duke.edu) to finish this lab.**"]},{"cell_type":"code","execution_count":null,"id":"9e934ed1","metadata":{"scrolled":false,"id":"9e934ed1","outputId":"cebabb46-2a68-45ae-c9fa-cf40d924779d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: snscrape in /Users/rongqibei/opt/anaconda3/lib/python3.8/site-packages (0.4.3.20220106)\n","Requirement already satisfied: pytz in /Users/rongqibei/opt/anaconda3/lib/python3.8/site-packages (from snscrape) (2021.1)\n","Requirement already satisfied: requests[socks] in /Users/rongqibei/opt/anaconda3/lib/python3.8/site-packages (from snscrape) (2.25.1)\n","Requirement already satisfied: beautifulsoup4 in /Users/rongqibei/opt/anaconda3/lib/python3.8/site-packages (from snscrape) (4.9.3)\n","Requirement already satisfied: filelock in /Users/rongqibei/opt/anaconda3/lib/python3.8/site-packages (from snscrape) (3.6.0)\n","Requirement already satisfied: lxml in /Users/rongqibei/opt/anaconda3/lib/python3.8/site-packages (from snscrape) (4.6.3)\n","Requirement already satisfied: soupsieve>1.2 in /Users/rongqibei/opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4->snscrape) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/rongqibei/opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape) (2020.12.5)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/rongqibei/opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape) (1.26.4)\n","Requirement already satisfied: idna<3,>=2.5 in /Users/rongqibei/opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /Users/rongqibei/opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape) (4.0.0)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/rongqibei/opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape) (1.7.1)\n"]}],"source":["# install snscrape using pip\n","#important notice: require Python version 3.8+\n","#if <3.8, try \"conda update python=[target version]\" or create new conda env with \"conda create -n [name you like] python=[target version]\" in command line\n","!pip3 install snscrape\n","\n","#or\n","#pip3 install git+https://github.com/JustAnotherArchivist/snscrape.git\n","\n","#more details please refer to the github link: https://github.com/JustAnotherArchivist/snscrape\n"]},{"cell_type":"code","execution_count":null,"id":"d4c6e7d6","metadata":{"id":"d4c6e7d6"},"outputs":[],"source":["# import libraries\n","import snscrape.modules.twitter as sntwitter\n","import snscrape.modules.twitter\n","\n","import pandas as pd"]},{"cell_type":"markdown","id":"b0e58a2f","metadata":{"id":"b0e58a2f"},"source":["## 1. Overview\n","\n","In today's lab, we are going to use snscrape to scrape data from twitter. You will learn to scrape tweets from:\n","1. text search query\n","2. a specific user\n","\n","Snscrape provides many useful attributes available through snscrape Twitter object [(Beck, 2020)](https://betterprogramming.pub/how-to-scrape-tweets-with-snscrape-90124ed006af). You might need some of the above attributes when scraping data for your group projects.\n","* url: Permalink pointing to tweet location\n","* date: The date on which the tweet was created\n","* content: The text content of the tweet\n","* id: The id of the tweet\n","* user: User object containing the following data: username, displayname, id, description, descriptionUrls, verified, created, followersCount, friendsCount, statusesCount, favouritesCount, listedCount, location, protected, linkUrl, profileImageUrl, profileBannerUrl\n","* replyCount: The count of replies\n","* retweetCount: The count of retweets\n","* likeCount: The count of likes\n","* quoteCount: The count of users that quoted the tweet and replied\n","* conversationId: Appears to be the same as tweet id\n","* lang: Machine generated, assumed language of the tweet\n","* source: Where tweet was posted from (e.g., iPhone, Andriod, etc.)\n","* retweetedTweet: The id of the original tweet (if it is a retweet)\n","* mentionedUsers: The user objects of any mentioned user in the tweet"]},{"cell_type":"markdown","id":"81ac6c0f","metadata":{"id":"81ac6c0f"},"source":["##  2. Scrape tweets from a text search query\n","\n","The queried text could be certain keywords and hashtags."]},{"cell_type":"markdown","id":"4a6bf7a5","metadata":{"id":"4a6bf7a5"},"source":["###  Example 1\n","In this example, we scrape 10 tweets with the **#stopasianhate** hashtag with at least three likes `min_faves:3`. We save the scraped tweets with its posted date `tweet.date`, text content `tweet.content`, poster's location `tweet.user.location`, and the number of received likes `tweet.likeCount` into a dataframe."]},{"cell_type":"code","execution_count":null,"id":"bcd716e6","metadata":{"id":"bcd716e6"},"outputs":[],"source":["# create a list to append twitter data to\n","tweets_list1 = []\n","\n","# use TwitterSearchScraper to scrape data and append tweets to list\n","for i, tweet in enumerate(sntwitter.TwitterSearchScraper('#stopasianhate since:2021-03-15 until:2021-03-16 min_faves:3').get_items()):\n","    if i >= 10:\n","        break\n","    tweets_list1.append([tweet.date,  tweet.content, tweet.user.location, tweet.likeCount])"]},{"cell_type":"code","execution_count":null,"id":"86960147","metadata":{"id":"86960147","outputId":"cfcda3fd-f16c-43a6-df57-ef0e14ff0fec"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Date</th>\n","      <th>Content</th>\n","      <th>User's location</th>\n","      <th># of likes</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2021-03-15 23:40:23+00:00</td>\n","      <td>Nobody can stop @JoeBiden @FLOTUS ! #vaccine n...</td>\n","      <td>Âè∞ÂåóÂ∏Ç, Âè∞ÁÅ£</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2021-03-15 23:36:27+00:00</td>\n","      <td>A big thank you to the entire community for su...</td>\n","      <td>San Francisco, CA</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2021-03-15 23:28:45+00:00</td>\n","      <td>@BigBosSebas @rayvolpe nice we talking about #...</td>\n","      <td>Los Angeles, CA</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2021-03-15 23:08:56+00:00</td>\n","      <td>@CeFaanKim @Syissle @JoshHartmann Thank you  @...</td>\n","      <td>Los Angeles &amp; New York</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2021-03-15 23:06:24+00:00</td>\n","      <td>Remember their faces. Remember their names. #S...</td>\n","      <td></td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2021-03-15 23:04:32+00:00</td>\n","      <td>1st step to solving a problem is acknowledging...</td>\n","      <td>San Antonio, TX</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2021-03-15 22:48:26+00:00</td>\n","      <td>Grateful for all my #YeunBuns, but let‚Äôs keep ...</td>\n","      <td>Los Angeles, CA</td>\n","      <td>1626</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>2021-03-15 22:35:14+00:00</td>\n","      <td>We condemn the hate crimes and discrimination ...</td>\n","      <td>Worldwide</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>2021-03-15 22:28:09+00:00</td>\n","      <td>When you have a moment please read this thread...</td>\n","      <td>Atlanta, GA</td>\n","      <td>26</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>2021-03-15 22:24:12+00:00</td>\n","      <td>‚ÄúGo back to f****** communist China you b****!...</td>\n","      <td>New York City</td>\n","      <td>1142</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                       Date  \\\n","0 2021-03-15 23:40:23+00:00   \n","1 2021-03-15 23:36:27+00:00   \n","2 2021-03-15 23:28:45+00:00   \n","3 2021-03-15 23:08:56+00:00   \n","4 2021-03-15 23:06:24+00:00   \n","5 2021-03-15 23:04:32+00:00   \n","6 2021-03-15 22:48:26+00:00   \n","7 2021-03-15 22:35:14+00:00   \n","8 2021-03-15 22:28:09+00:00   \n","9 2021-03-15 22:24:12+00:00   \n","\n","                                             Content         User's location  \\\n","0  Nobody can stop @JoeBiden @FLOTUS ! #vaccine n...                 Âè∞ÂåóÂ∏Ç, Âè∞ÁÅ£   \n","1  A big thank you to the entire community for su...       San Francisco, CA   \n","2  @BigBosSebas @rayvolpe nice we talking about #...         Los Angeles, CA   \n","3  @CeFaanKim @Syissle @JoshHartmann Thank you  @...  Los Angeles & New York   \n","4  Remember their faces. Remember their names. #S...                           \n","5  1st step to solving a problem is acknowledging...         San Antonio, TX   \n","6  Grateful for all my #YeunBuns, but let‚Äôs keep ...         Los Angeles, CA   \n","7  We condemn the hate crimes and discrimination ...               Worldwide   \n","8  When you have a moment please read this thread...             Atlanta, GA   \n","9  ‚ÄúGo back to f****** communist China you b****!...           New York City   \n","\n","   # of likes  \n","0           4  \n","1           9  \n","2           5  \n","3          12  \n","4          20  \n","5           4  \n","6        1626  \n","7           6  \n","8          26  \n","9        1142  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# create a dataframe to view the result\n","tweets_df1 = pd.DataFrame(tweets_list1, columns=['Date', 'Content', 'User\\'s location', '# of likes'])\n","tweets_df1"]},{"cell_type":"markdown","id":"4515d5fc","metadata":{"id":"4515d5fc"},"source":["### Example 2\n","\n","In this example, we scrape tweets that contains keywords in the phrase **let's go duke** around the date of a basketball match. We save the scraped tweets with its posted date `tweet.date`, text content `tweet.content`, and the user's location `tweet.user.location` into a dataframe."]},{"cell_type":"code","execution_count":null,"id":"845106a7","metadata":{"id":"845106a7"},"outputs":[],"source":["# create a list to append twitter data to\n","tweets_list2 = []\n","\n","# use TwitterSearchScraper to scrape data and append tweets to list\n","for i, tweet in enumerate(sntwitter.TwitterSearchScraper('let\\'s go duke since:2021-11-12 until:2021-11-13').get_items()):\n","    tweets_list2.append([tweet.date, tweet.content, tweet.user.location])"]},{"cell_type":"code","execution_count":null,"id":"78570e1d","metadata":{"scrolled":true,"id":"78570e1d","outputId":"c871b842-4226-4c99-ce73-e339dc610dac"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Date</th>\n","      <th>Content</th>\n","      <th>User's location</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2021-11-12 23:59:21+00:00</td>\n","      <td>Let's go Duke ü§ò</td>\n","      <td>My Own Personal Hell</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2021-11-12 23:58:46+00:00</td>\n","      <td>Roach\\nKeels\\nMoore\\nBanchero\\nWilliams\\n\\nGra...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2021-11-12 23:57:29+00:00</td>\n","      <td>üëèüèª Let‚Äôs üëèüèª Go ‚úäüèª Duke! üîµüòà https://t.co/H589Xp...</td>\n","      <td>Germantown, MD</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2021-11-12 23:56:40+00:00</td>\n","      <td>Let‚Äôs go Duke!! #BeatArmy</td>\n","      <td>#SI6HTS</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2021-11-12 23:55:37+00:00</td>\n","      <td>LET‚ÄôS GO DUKE!!! One last ride against where t...</td>\n","      <td>Raleigh, NC</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2021-11-12 23:54:42+00:00</td>\n","      <td>Let‚Äôs Go Dukeüëø https://t.co/beTdrGHzXW</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2021-11-12 23:53:25+00:00</td>\n","      <td>Let‚Äôs go Duke!!! üíôüòàüèÄ</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>2021-11-12 23:51:58+00:00</td>\n","      <td>LET‚ÄôS GO DUKE üòà</td>\n","      <td>252</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>2021-11-12 23:51:13+00:00</td>\n","      <td>Gametime baby let's go Duke https://t.co/lmXSV...</td>\n","      <td>San Antonio, TX</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>2021-11-12 23:48:41+00:00</td>\n","      <td>@DukeMBB Let's Go Duke!!! #HereComesDuke</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>2021-11-12 23:48:02+00:00</td>\n","      <td>It‚Äôs almost game time Let‚Äôs Go Duke #DukeNation</td>\n","      <td>Tampa, FL</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>2021-11-12 23:47:43+00:00</td>\n","      <td>@dukepbp @DukeMBB @ReedsJewelers @DukeATHLETIC...</td>\n","      <td>Harrisburg PA</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>2021-11-12 23:28:17+00:00</td>\n","      <td>@DukeMBB üî•üî•let‚Äôs go duke</td>\n","      <td>Uptown Dc</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>2021-11-12 23:26:28+00:00</td>\n","      <td>@DerrickWalkerJ2 Let's go ACC. Looking forward...</td>\n","      <td>Fort Lauderdale, FL</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>2021-11-12 23:25:53+00:00</td>\n","      <td>let‚Äôs go duke</td>\n","      <td>RGF Island</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>2021-11-12 23:25:14+00:00</td>\n","      <td>Soooooo Nice to be back in Cameron !!!!! Let‚Äôs...</td>\n","      <td>Durham, NC</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>2021-11-12 23:24:31+00:00</td>\n","      <td>Let‚Äôs Go Duke</td>\n","      <td>North KaK</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>2021-11-12 22:52:40+00:00</td>\n","      <td>@GameDayCharlie @ABC11_WTVD @DukeMBB Let's go ...</td>\n","      <td>The Greater Triangle Area</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>2021-11-12 22:34:52+00:00</td>\n","      <td>GAME DAY !!! #HereComesDuke !!!\\nüèÄüèÄüèÄ‚Ä¶vs Army\\n...</td>\n","      <td>Harrisburg PA</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>2021-11-12 22:07:10+00:00</td>\n","      <td>We Have An All New Wwe Smackdown And An All Ae...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>2021-11-12 21:47:08+00:00</td>\n","      <td>@DukeWBB Let's go duke!!!!</td>\n","      <td>in my right mind :)</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>2021-11-12 21:44:02+00:00</td>\n","      <td>@JCrossover @Pp_doesit Me and Paolo B day twin...</td>\n","      <td>206üìç</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>2021-11-12 21:37:59+00:00</td>\n","      <td>Let‚Äôs go Duke !</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>2021-11-12 21:03:02+00:00</td>\n","      <td>Duke will be wearing their gray/gold/blue jers...</td>\n","      <td>‚ú®Illinois ‚ú®</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>2021-11-12 19:15:46+00:00</td>\n","      <td>@DukeMBB Let‚Äôs GO DUKE !!!</td>\n","      <td>Harrisburg PA</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>2021-11-12 17:39:22+00:00</td>\n","      <td>So stoked to get this Veterans Tribute Showcas...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>2021-11-12 16:53:51+00:00</td>\n","      <td>Let‚Äôs Go Duke! #Gameday</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>2021-11-12 16:46:14+00:00</td>\n","      <td>@sobberatduke @DukeMBB Let‚Äôs go Duke!</td>\n","      <td>Dallas, TX</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>2021-11-12 16:01:21+00:00</td>\n","      <td>Same tournament 2 years ago and still playing ...</td>\n","      <td>Atlanta, GA</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>2021-11-12 15:56:33+00:00</td>\n","      <td>Let‚Äôs go Duke! https://t.co/dQvVu3wGe8</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>2021-11-12 15:35:46+00:00</td>\n","      <td>Girls‚Äô Diving\\nState\\nKalina‚Äôs first dive!\\nGr...</td>\n","      <td>Elmhurst, IL</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>2021-11-12 15:34:19+00:00</td>\n","      <td>Let‚Äôs go @DaytonWBB‚ÄºÔ∏è\\n\\nBe sure to show up to...</td>\n","      <td>Dayton, OH</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>2021-11-12 15:33:14+00:00</td>\n","      <td>Let‚Äôs go Duke! https://t.co/FaXmZIXuO0</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>2021-11-12 15:05:30+00:00</td>\n","      <td>Duke bound!  Here we come, Cameron. Thank you ...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>2021-11-12 14:55:16+00:00</td>\n","      <td>Happy Birthday Paolo üòàüòàüíôüíô Let‚Äôs go Duke #GameD...</td>\n","      <td>Tampa, FL</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>2021-11-12 14:14:24+00:00</td>\n","      <td>üîµ Gameday! Let's Go Duke! üîµ\\n\\nüèÜ Veterans Day ...</td>\n","      <td>Durham, NC</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>2021-11-12 14:01:05+00:00</td>\n","      <td>@Zlanier21 @sarahlongthorne Dukes up. SJ let‚Äôs...</td>\n","      <td>Seattle, Washington, USA</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>2021-11-12 13:57:25+00:00</td>\n","      <td>@Duke_kanbai_ @Eghyofbenin I think I need to s...</td>\n","      <td>iCONs Republic</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>2021-11-12 13:24:21+00:00</td>\n","      <td>It‚Äôs game day! \\n\\nLet‚Äôs go Duke!</td>\n","      <td>Vincent Ohio</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>2021-11-12 12:30:54+00:00</td>\n","      <td>@DukeMBB @UpdateDuke @DukeDigest @dukepbp @duk...</td>\n","      <td>World Wide</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>2021-11-12 07:54:04+00:00</td>\n","      <td>@Qu3stion_84 @DukeOfSexy @rainboom16 @HimuraHi...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>2021-11-12 02:03:14+00:00</td>\n","      <td>Happy Veterans Day!! First game in Cameron tom...</td>\n","      <td>Cameron Indoor Stadium</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>2021-11-12 00:29:59+00:00</td>\n","      <td>@TylerMeinerding Kenny Anderson throwback is ü§Æ...</td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                        Date  \\\n","0  2021-11-12 23:59:21+00:00   \n","1  2021-11-12 23:58:46+00:00   \n","2  2021-11-12 23:57:29+00:00   \n","3  2021-11-12 23:56:40+00:00   \n","4  2021-11-12 23:55:37+00:00   \n","5  2021-11-12 23:54:42+00:00   \n","6  2021-11-12 23:53:25+00:00   \n","7  2021-11-12 23:51:58+00:00   \n","8  2021-11-12 23:51:13+00:00   \n","9  2021-11-12 23:48:41+00:00   \n","10 2021-11-12 23:48:02+00:00   \n","11 2021-11-12 23:47:43+00:00   \n","12 2021-11-12 23:28:17+00:00   \n","13 2021-11-12 23:26:28+00:00   \n","14 2021-11-12 23:25:53+00:00   \n","15 2021-11-12 23:25:14+00:00   \n","16 2021-11-12 23:24:31+00:00   \n","17 2021-11-12 22:52:40+00:00   \n","18 2021-11-12 22:34:52+00:00   \n","19 2021-11-12 22:07:10+00:00   \n","20 2021-11-12 21:47:08+00:00   \n","21 2021-11-12 21:44:02+00:00   \n","22 2021-11-12 21:37:59+00:00   \n","23 2021-11-12 21:03:02+00:00   \n","24 2021-11-12 19:15:46+00:00   \n","25 2021-11-12 17:39:22+00:00   \n","26 2021-11-12 16:53:51+00:00   \n","27 2021-11-12 16:46:14+00:00   \n","28 2021-11-12 16:01:21+00:00   \n","29 2021-11-12 15:56:33+00:00   \n","30 2021-11-12 15:35:46+00:00   \n","31 2021-11-12 15:34:19+00:00   \n","32 2021-11-12 15:33:14+00:00   \n","33 2021-11-12 15:05:30+00:00   \n","34 2021-11-12 14:55:16+00:00   \n","35 2021-11-12 14:14:24+00:00   \n","36 2021-11-12 14:01:05+00:00   \n","37 2021-11-12 13:57:25+00:00   \n","38 2021-11-12 13:24:21+00:00   \n","39 2021-11-12 12:30:54+00:00   \n","40 2021-11-12 07:54:04+00:00   \n","41 2021-11-12 02:03:14+00:00   \n","42 2021-11-12 00:29:59+00:00   \n","\n","                                              Content  \\\n","0                                     Let's go Duke ü§ò   \n","1   Roach\\nKeels\\nMoore\\nBanchero\\nWilliams\\n\\nGra...   \n","2   üëèüèª Let‚Äôs üëèüèª Go ‚úäüèª Duke! üîµüòà https://t.co/H589Xp...   \n","3                           Let‚Äôs go Duke!! #BeatArmy   \n","4   LET‚ÄôS GO DUKE!!! One last ride against where t...   \n","5              Let‚Äôs Go Dukeüëø https://t.co/beTdrGHzXW   \n","6                                Let‚Äôs go Duke!!! üíôüòàüèÄ   \n","7                                     LET‚ÄôS GO DUKE üòà   \n","8   Gametime baby let's go Duke https://t.co/lmXSV...   \n","9            @DukeMBB Let's Go Duke!!! #HereComesDuke   \n","10    It‚Äôs almost game time Let‚Äôs Go Duke #DukeNation   \n","11  @dukepbp @DukeMBB @ReedsJewelers @DukeATHLETIC...   \n","12                           @DukeMBB üî•üî•let‚Äôs go duke   \n","13  @DerrickWalkerJ2 Let's go ACC. Looking forward...   \n","14                                      let‚Äôs go duke   \n","15  Soooooo Nice to be back in Cameron !!!!! Let‚Äôs...   \n","16                                      Let‚Äôs Go Duke   \n","17  @GameDayCharlie @ABC11_WTVD @DukeMBB Let's go ...   \n","18  GAME DAY !!! #HereComesDuke !!!\\nüèÄüèÄüèÄ‚Ä¶vs Army\\n...   \n","19  We Have An All New Wwe Smackdown And An All Ae...   \n","20                         @DukeWBB Let's go duke!!!!   \n","21  @JCrossover @Pp_doesit Me and Paolo B day twin...   \n","22                                    Let‚Äôs go Duke !   \n","23  Duke will be wearing their gray/gold/blue jers...   \n","24                         @DukeMBB Let‚Äôs GO DUKE !!!   \n","25  So stoked to get this Veterans Tribute Showcas...   \n","26                            Let‚Äôs Go Duke! #Gameday   \n","27              @sobberatduke @DukeMBB Let‚Äôs go Duke!   \n","28  Same tournament 2 years ago and still playing ...   \n","29             Let‚Äôs go Duke! https://t.co/dQvVu3wGe8   \n","30  Girls‚Äô Diving\\nState\\nKalina‚Äôs first dive!\\nGr...   \n","31  Let‚Äôs go @DaytonWBB‚ÄºÔ∏è\\n\\nBe sure to show up to...   \n","32             Let‚Äôs go Duke! https://t.co/FaXmZIXuO0   \n","33  Duke bound!  Here we come, Cameron. Thank you ...   \n","34  Happy Birthday Paolo üòàüòàüíôüíô Let‚Äôs go Duke #GameD...   \n","35  üîµ Gameday! Let's Go Duke! üîµ\\n\\nüèÜ Veterans Day ...   \n","36  @Zlanier21 @sarahlongthorne Dukes up. SJ let‚Äôs...   \n","37  @Duke_kanbai_ @Eghyofbenin I think I need to s...   \n","38                  It‚Äôs game day! \\n\\nLet‚Äôs go Duke!   \n","39  @DukeMBB @UpdateDuke @DukeDigest @dukepbp @duk...   \n","40  @Qu3stion_84 @DukeOfSexy @rainboom16 @HimuraHi...   \n","41  Happy Veterans Day!! First game in Cameron tom...   \n","42  @TylerMeinerding Kenny Anderson throwback is ü§Æ...   \n","\n","              User's location  \n","0        My Own Personal Hell  \n","1                              \n","2              Germantown, MD  \n","3                     #SI6HTS  \n","4                 Raleigh, NC  \n","5                              \n","6                              \n","7                         252  \n","8             San Antonio, TX  \n","9                              \n","10                  Tampa, FL  \n","11              Harrisburg PA  \n","12                  Uptown Dc  \n","13        Fort Lauderdale, FL  \n","14                 RGF Island  \n","15                 Durham, NC  \n","16                  North KaK  \n","17  The Greater Triangle Area  \n","18              Harrisburg PA  \n","19                             \n","20        in my right mind :)  \n","21                       206üìç  \n","22                             \n","23                ‚ú®Illinois ‚ú®  \n","24              Harrisburg PA  \n","25                             \n","26                             \n","27                 Dallas, TX  \n","28                Atlanta, GA  \n","29                             \n","30               Elmhurst, IL  \n","31                 Dayton, OH  \n","32                             \n","33                             \n","34                  Tampa, FL  \n","35                 Durham, NC  \n","36   Seattle, Washington, USA  \n","37             iCONs Republic  \n","38               Vincent Ohio  \n","39                 World Wide  \n","40                             \n","41    Cameron Indoor Stadium   \n","42                             "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# create a dataframe to view the result\n","tweets_df2 = pd.DataFrame(tweets_list2, columns=['Date', 'Content', 'User\\'s location'])\n","tweets_df2"]},{"cell_type":"markdown","id":"16ab0fc4","metadata":{"id":"16ab0fc4"},"source":["## 3. Scrape tweets from a specific user"]},{"cell_type":"markdown","id":"506268da","metadata":{"id":"506268da"},"source":["### Example 3"]},{"cell_type":"markdown","id":"f5a6f869","metadata":{"id":"f5a6f869"},"source":["In this example, we scrape the latest 100 tweets posted by the Twitter user **@DukeKunshan**. We save the scraped tweets with its posted date `tweet.date`, text content `tweet.content`, and username `tweet.user.username` into a dataframe."]},{"cell_type":"code","execution_count":null,"id":"ad8bddc2","metadata":{"id":"ad8bddc2"},"outputs":[],"source":["# create a list to append twitter data to\n","tweets_list3 = []\n","\n","# use TwitterSearchScraper to scrape data and append tweets to list\n","for i, tweet in enumerate(sntwitter.TwitterSearchScraper('from:DukeKunshan').get_items()):\n","    if i >= 100:\n","        break\n","    tweets_list3.append([tweet.date, tweet.content, tweet.user.username])"]},{"cell_type":"code","execution_count":null,"id":"34b178c1","metadata":{"scrolled":false,"id":"34b178c1","outputId":"4f2ae593-6f01-4616-e481-5810c35b1bce"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Date</th>\n","      <th>Content</th>\n","      <th>Username</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2022-03-24 01:00:05+00:00</td>\n","      <td>üì¢ Starting tomorrow: Discrete Math Seminar\\n\\n...</td>\n","      <td>DukeKunshan</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2022-03-23 02:30:02+00:00</td>\n","      <td>üì¢  Starting tomorrow: Duke Kunshan University ...</td>\n","      <td>DukeKunshan</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2022-03-16 02:00:05+00:00</td>\n","      <td>üì¢  Save the date: Duke Kunshan University 2022...</td>\n","      <td>DukeKunshan</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2022-03-15 15:30:12+00:00</td>\n","      <td>Check out these photos of construction progres...</td>\n","      <td>DukeKunshan</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2022-03-14 11:00:10+00:00</td>\n","      <td>üì¢ GHYLP Global Health Seminar Series 2022\\n\\nüóì...</td>\n","      <td>DukeKunshan</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>2021-09-20 03:30:02+00:00</td>\n","      <td>A new art book co-edited by Duke Kunshan profe...</td>\n","      <td>DukeKunshan</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>2021-09-19 01:00:10+00:00</td>\n","      <td>Wishing everyone a happy Mid-Autumn Festival!\\...</td>\n","      <td>DukeKunshan</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>2021-09-16 07:00:05+00:00</td>\n","      <td>üì¢  Starting tomorrow: DKU Career Fair | Fall 2...</td>\n","      <td>DukeKunshan</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>2021-09-13 03:30:01+00:00</td>\n","      <td>After Covid-19 made international travel almos...</td>\n","      <td>DukeKunshan</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>2021-09-09 07:30:02+00:00</td>\n","      <td>üì¢  Happening tomorrow: DSRC Seminar | New chal...</td>\n","      <td>DukeKunshan</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows √ó 3 columns</p>\n","</div>"],"text/plain":["                        Date  \\\n","0  2022-03-24 01:00:05+00:00   \n","1  2022-03-23 02:30:02+00:00   \n","2  2022-03-16 02:00:05+00:00   \n","3  2022-03-15 15:30:12+00:00   \n","4  2022-03-14 11:00:10+00:00   \n","..                       ...   \n","95 2021-09-20 03:30:02+00:00   \n","96 2021-09-19 01:00:10+00:00   \n","97 2021-09-16 07:00:05+00:00   \n","98 2021-09-13 03:30:01+00:00   \n","99 2021-09-09 07:30:02+00:00   \n","\n","                                              Content     Username  \n","0   üì¢ Starting tomorrow: Discrete Math Seminar\\n\\n...  DukeKunshan  \n","1   üì¢  Starting tomorrow: Duke Kunshan University ...  DukeKunshan  \n","2   üì¢  Save the date: Duke Kunshan University 2022...  DukeKunshan  \n","3   Check out these photos of construction progres...  DukeKunshan  \n","4   üì¢ GHYLP Global Health Seminar Series 2022\\n\\nüóì...  DukeKunshan  \n","..                                                ...          ...  \n","95  A new art book co-edited by Duke Kunshan profe...  DukeKunshan  \n","96  Wishing everyone a happy Mid-Autumn Festival!\\...  DukeKunshan  \n","97  üì¢  Starting tomorrow: DKU Career Fair | Fall 2...  DukeKunshan  \n","98  After Covid-19 made international travel almos...  DukeKunshan  \n","99  üì¢  Happening tomorrow: DSRC Seminar | New chal...  DukeKunshan  \n","\n","[100 rows x 3 columns]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# create a dataframe to view the result\n","tweets_df3 = pd.DataFrame(tweets_list3, columns=['Date', 'Content', 'Username'])\n","tweets_df3"]},{"cell_type":"code","execution_count":null,"id":"a85c16e4","metadata":{"id":"a85c16e4"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"2eb8c36f","metadata":{"id":"2eb8c36f"},"source":["# Optional: Scraping Weibo data"]},{"cell_type":"markdown","id":"b42b0c56","metadata":{"id":"b42b0c56"},"source":["If your project's topic falls in the environment of Chinese society, you may need to scrape data from [Weibo](http://www.weibo.com/) instead of Twitter. A simple, easy-to-use API can be found [here](https://github.com/dataabc/weibo-search). "]},{"cell_type":"code","execution_count":null,"id":"8d904af5","metadata":{"id":"8d904af5"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"STATS401_lab3_data_scraper.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}